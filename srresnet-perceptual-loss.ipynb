{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import InterpolationMode\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport PIL","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.561195Z","iopub.execute_input":"2022-02-19T17:13:21.561452Z","iopub.status.idle":"2022-02-19T17:13:21.566262Z","shell.execute_reply.started":"2022-02-19T17:13:21.561423Z","shell.execute_reply":"2022-02-19T17:13:21.565451Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nannotations_file = pd.read_csv(\"../input/the-oxfordiiit-pet-dataset/annotations/annotations/list.txt\",\n                               sep = ' ',\n                               on_bad_lines='skip')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.568148Z","iopub.execute_input":"2022-02-19T17:13:21.568571Z","iopub.status.idle":"2022-02-19T17:13:21.587835Z","shell.execute_reply.started":"2022-02-19T17:13:21.568535Z","shell.execute_reply":"2022-02-19T17:13:21.587180Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"annotations_file.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.590362Z","iopub.execute_input":"2022-02-19T17:13:21.590548Z","iopub.status.idle":"2022-02-19T17:13:21.604339Z","shell.execute_reply.started":"2022-02-19T17:13:21.590525Z","shell.execute_reply":"2022-02-19T17:13:21.603711Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"annotations_file = annotations_file.drop(labels=range(3), axis=0)\nannotations_file.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.605408Z","iopub.execute_input":"2022-02-19T17:13:21.605782Z","iopub.status.idle":"2022-02-19T17:13:21.620470Z","shell.execute_reply.started":"2022-02-19T17:13:21.605747Z","shell.execute_reply":"2022-02-19T17:13:21.619839Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"len(annotations_file)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.622720Z","iopub.execute_input":"2022-02-19T17:13:21.623183Z","iopub.status.idle":"2022-02-19T17:13:21.628211Z","shell.execute_reply.started":"2022-02-19T17:13:21.623147Z","shell.execute_reply":"2022-02-19T17:13:21.627458Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"annotations_file.iloc[0,0]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.629476Z","iopub.execute_input":"2022-02-19T17:13:21.630785Z","iopub.status.idle":"2022-02-19T17:13:21.638461Z","shell.execute_reply.started":"2022-02-19T17:13:21.630748Z","shell.execute_reply":"2022-02-19T17:13:21.637844Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dir_path = '../input/the-oxfordiiit-pet-dataset/images/images/'\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.639527Z","iopub.execute_input":"2022-02-19T17:13:21.641778Z","iopub.status.idle":"2022-02-19T17:13:21.646163Z","shell.execute_reply.started":"2022-02-19T17:13:21.641739Z","shell.execute_reply":"2022-02-19T17:13:21.645119Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self,annotations_file,img_dir,transform = None):\n        self.annotations = annotations_file\n        self.img_dir = img_dir\n        self.transforms = transform\n        \n    #len simply returns the length of the dataset    \n    def __len__(self):\n        return len(self.annotations)\n    \n    #getitem returns the (x,y) pair at a particular index\n    def __getitem__(self,index):\n        img_path = f\"{self.img_dir}/{self.annotations.iloc[index,0]}.jpg\"\n        img = PIL.Image.open(img_path).convert('RGB')\n      \n        label = torch.tensor(int(self.annotations.iloc[index,1]))\n        \n        if self.transforms:\n            high_res_image = self.transforms[0](img)\n            low_res_image = self.transforms[1](img)\n            \n        return (high_res_image,low_res_image),label","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.647859Z","iopub.execute_input":"2022-02-19T17:13:21.648188Z","iopub.status.idle":"2022-02-19T17:13:21.656178Z","shell.execute_reply.started":"2022-02-19T17:13:21.648155Z","shell.execute_reply":"2022-02-19T17:13:21.655335Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class res_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,stride):\n        super().__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,\n                                             padding = kernel_size // 2),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.PReLU(out_channels),\n                                   nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,\n                                             padding = kernel_size // 2),\n                                   nn.BatchNorm2d(out_channels))\n    \n    def forward(self,x):\n        return self.block(x)\n                                   \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.657608Z","iopub.execute_input":"2022-02-19T17:13:21.658121Z","iopub.status.idle":"2022-02-19T17:13:21.665873Z","shell.execute_reply.started":"2022-02-19T17:13:21.658083Z","shell.execute_reply":"2022-02-19T17:13:21.665202Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class p_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,stride,scale):\n        super().__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,padding = kernel_size // 2),\n                                   nn.PixelShuffle(scale),\n                                   nn.PReLU(out_channels // (scale**2)))\n                                   \n    \n    def forward(self,x):\n        return self.block(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.667152Z","iopub.execute_input":"2022-02-19T17:13:21.667593Z","iopub.status.idle":"2022-02-19T17:13:21.676025Z","shell.execute_reply.started":"2022-02-19T17:13:21.667557Z","shell.execute_reply":"2022-02-19T17:13:21.675262Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"loss_network = torchvision.models.vgg16(pretrained = False)\nprint(loss_network)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:21.677400Z","iopub.execute_input":"2022-02-19T17:13:21.677916Z","iopub.status.idle":"2022-02-19T17:13:23.786159Z","shell.execute_reply.started":"2022-02-19T17:13:21.677882Z","shell.execute_reply":"2022-02-19T17:13:23.785353Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"loss_network = torchvision.models.vgg16(pretrained = True)            ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:23.787325Z","iopub.execute_input":"2022-02-19T17:13:23.787672Z","iopub.status.idle":"2022-02-19T17:13:47.410892Z","shell.execute_reply.started":"2022-02-19T17:13:23.787618Z","shell.execute_reply":"2022-02-19T17:13:47.410155Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class feature_loss_net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.loss_network = loss_network\n        \n    def forward(self,x):\n        output_dict = {}\n        \n        for idx in range(12):\n            x = self.loss_network.features[idx](x)\n            output_dict[idx] = x\n            \n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:47.412225Z","iopub.execute_input":"2022-02-19T17:13:47.412471Z","iopub.status.idle":"2022-02-19T17:13:47.418425Z","shell.execute_reply.started":"2022-02-19T17:13:47.412438Z","shell.execute_reply":"2022-02-19T17:13:47.417674Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class SRResNet(pl.LightningModule):\n    def __init__(self,lr,img_channels,img_size,depth,shuffle_scale):\n        super().__init__()\n        self.save_hyperparameters()      \n        \n        self.conv1 = nn.Sequential(nn.Conv2d(img_channels,64,kernel_size = 9, stride = 1,padding = 4),\n                                   nn.PReLU(64))\n        \n        self.residual_blocks = nn.ModuleList([\n                               res_block(64,64,3,1)\n                            \n                               for _ in range(depth)\n        ])\n        \n        self.conv2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 3//2),\n                                   nn.BatchNorm2d(64))\n        \n        self.pshuffle = nn.ModuleList(\n                                       [\n                                           p_block(64,256,3,1,2),\n                                           p_block(64,256,3,1,2)                                           \n                                       ]\n        )\n        \n        self.conv3 = nn.Conv2d(64,3,kernel_size = 9,stride = 1, padding = 9//2)\n        \n        self.loss_params = nn.Parameter(torch.ones(1,5))\n        \n    def forward(self,x):    \n        N,C,H,W = x.shape\n        H_out = H*2*self.hparams.shuffle_scale\n        W_out = W*2*self.hparams.shuffle_scale\n        \n        x = self.conv1(x)\n        conv1_output = x\n        for block in self.residual_blocks:\n            x  = block(x) + x\n        \n        x = self.conv2(x) + conv1_output\n        \n        for block in self.pshuffle:\n            x  = block(x)\n            \n        x = self.conv3(x)\n        x = x.view(N,C,H_out,W_out)     \n        \n        return x           \n  \n    def loss_fn(self,out_dict1,out_dict2):\n        criterion = nn.MSELoss(reduction = 'mean')\n        loss_params = nn.functional.softmax(self.loss_params,dim = 1)\n        loss = (criterion(out_dict1[1],out_dict2[1])*loss_params[0][0] +\n               criterion(out_dict1[3],out_dict2[3])*loss_params[0][1] + \n               criterion(out_dict1[6],out_dict2[6])*loss_params[0][2] +\n               criterion(out_dict1[8],out_dict2[8])*loss_params[0][3] + \n               criterion(out_dict1[11],out_dict2[11])*loss_params[0][4])\n        return loss/5\n            \n         \n    \n    def training_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        y_hat = self(low_res_image)\n        \n        loss_net = feature_loss_net().to(self.device)\n        loss_net.eval()        \n        \n        output_dict1 = loss_net(high_res_image)\n        output_dict2 = loss_net(self(low_res_image))\n        \n        \n        \n        loss = self.loss_fn(output_dict1,output_dict2)\n        return loss\n    \n    \n    \n    def validation_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        y_hat = self(low_res_image)\n        \n        loss_net = feature_loss_net().to(self.device)\n        loss_net.eval()\n        \n        output_dict1 = loss_net(high_res_image)\n        output_dict2 = loss_net(self(low_res_image))\n        \n        \n        \n        loss = self.loss_fn(output_dict1,output_dict2)\n        self.log(\"val_loss\",loss)\n        \n    def test_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        y_hat = self(low_res_image)\n        \n        loss_net = feature_loss_net().to(self.device)\n        loss_net.eval()\n        \n        output_dict1 = loss_net(high_res_image)\n        output_dict2 = loss_net(self(low_res_image))\n        \n        \n        \n        loss = self.loss_fn(output_dict1,output_dict2)\n        self.log(\"test_loss\",loss)\n\n        \n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(),lr = self.hparams.lr)\n        return optimizer\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:47.422115Z","iopub.execute_input":"2022-02-19T17:13:47.422926Z","iopub.status.idle":"2022-02-19T17:13:47.443561Z","shell.execute_reply.started":"2022-02-19T17:13:47.422890Z","shell.execute_reply":"2022-02-19T17:13:47.442890Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\n    resize_transform = transforms.Compose([\n                                    transforms.Resize((320,320)),\n                                    transforms.ToTensor()\n                               ]\n    )\n\n    transform = transforms.Compose([\n                                    transforms.Resize((80,80)),\n                                    transforms.ToTensor()\n                               ]\n    )\n\n\n    dataset = CustomDataset(annotations_file = annotations_file,\n                            img_dir = dir_path ,\n                            transform = [resize_transform,transform])       \n\n    dataset_len = len(dataset)\n    train_set_len = int(0.7*dataset_len)\n    val_set_len = int(0.15*dataset_len)\n    test_set_len  = dataset_len - train_set_len - val_set_len\n\n    train_set,val_set,test_set = torch.utils.data.random_split(dataset,\n                                                               [train_set_len,val_set_len,test_set_len],\n                                                               generator=torch.Generator().manual_seed(43))\n\n    train_loader = DataLoader(train_set,batch_size = 16)\n    val_loader = DataLoader(val_set,batch_size = 16)\n    test_loader = DataLoader(test_set,batch_size = 16)        \n\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:47.444822Z","iopub.execute_input":"2022-02-19T17:13:47.445356Z","iopub.status.idle":"2022-02-19T17:13:47.461498Z","shell.execute_reply.started":"2022-02-19T17:13:47.445277Z","shell.execute_reply":"2022-02-19T17:13:47.460865Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for batch in test_loader:\n        (x1,x2), _ = batch\n        x1= x1[0:3]\n        x1.unsqueeze(0)\n        grid = torchvision.utils.make_grid(x1)\n        high_res_imgs = grid\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:47.462914Z","iopub.execute_input":"2022-02-19T17:13:47.463280Z","iopub.status.idle":"2022-02-19T17:13:47.729518Z","shell.execute_reply.started":"2022-02-19T17:13:47.463227Z","shell.execute_reply":"2022-02-19T17:13:47.728773Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n                                                       patience = 2,\n                                                       mode = 'min')\n\ncheckpoint = pl.callbacks.ModelCheckpoint(dirpath = 'saved_ckpts',\n                                          monitor = 'val_loss')\n\n\n\nmodel = SRResNet(lr = 1e-4,\n              img_channels = 3,\n              img_size = 80,\n              depth = 12,\n              shuffle_scale = 2)\ntrainer = pl.Trainer(gpus = 1,\n                     precision = 16,\n                     callbacks = [early_stopping,checkpoint],\n                     max_epochs = 10)\n\ntrainer.fit(model,train_loader,val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:13:47.730703Z","iopub.execute_input":"2022-02-19T17:13:47.730963Z","iopub.status.idle":"2022-02-19T18:09:40.125072Z","shell.execute_reply.started":"2022-02-19T17:13:47.730930Z","shell.execute_reply":"2022-02-19T18:09:40.124195Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model = SRResNet.load_from_checkpoint(checkpoint.best_model_path)\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        (x1,x2), _ = batch\n        x2 = x2[0:3]\n        output_imgs = model(x2)\n        output_imgs.unsqueeze(0)\n        grid = torchvision.utils.make_grid(output_imgs)\n        output_imgs = grid\n        break\n\n\ntrainer.test(model = model,dataloaders = test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T18:09:40.126462Z","iopub.execute_input":"2022-02-19T18:09:40.127217Z","iopub.status.idle":"2022-02-19T18:09:59.843315Z","shell.execute_reply.started":"2022-02-19T18:09:40.127177Z","shell.execute_reply":"2022-02-19T18:09:59.842563Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.imshow(high_res_imgs.permute(1,2,0))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T18:09:59.844494Z","iopub.execute_input":"2022-02-19T18:09:59.844826Z","iopub.status.idle":"2022-02-19T18:10:00.296431Z","shell.execute_reply.started":"2022-02-19T18:09:59.844787Z","shell.execute_reply":"2022-02-19T18:10:00.295823Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nplt.imshow(output_imgs.permute(1,2,0))","metadata":{"execution":{"iopub.status.busy":"2022-02-19T18:13:32.207242Z","iopub.execute_input":"2022-02-19T18:13:32.207632Z","iopub.status.idle":"2022-02-19T18:13:33.152445Z","shell.execute_reply.started":"2022-02-19T18:13:32.207571Z","shell.execute_reply":"2022-02-19T18:13:33.151672Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-19T18:14:28.753458Z","iopub.execute_input":"2022-02-19T18:14:28.753738Z","iopub.status.idle":"2022-02-19T18:14:29.187192Z","shell.execute_reply.started":"2022-02-19T18:14:28.753707Z","shell.execute_reply":"2022-02-19T18:14:29.186587Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}