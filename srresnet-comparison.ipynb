{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import InterpolationMode\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nimport PIL\nimport ignite.metrics\nimport numpy as np\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:38.854236Z","iopub.execute_input":"2022-02-19T17:46:38.8548Z","iopub.status.idle":"2022-02-19T17:46:43.136718Z","shell.execute_reply.started":"2022-02-19T17:46:38.854687Z","shell.execute_reply":"2022-02-19T17:46:43.135337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nannotations_file = pd.read_csv(\"../input/the-oxfordiiit-pet-dataset/annotations/annotations/list.txt\",\n                               sep = ' ',\n                               on_bad_lines='skip')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.140089Z","iopub.execute_input":"2022-02-19T17:46:43.140426Z","iopub.status.idle":"2022-02-19T17:46:43.17241Z","shell.execute_reply.started":"2022-02-19T17:46:43.14038Z","shell.execute_reply":"2022-02-19T17:46:43.171525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_file.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.173684Z","iopub.execute_input":"2022-02-19T17:46:43.173911Z","iopub.status.idle":"2022-02-19T17:46:43.201786Z","shell.execute_reply.started":"2022-02-19T17:46:43.173877Z","shell.execute_reply":"2022-02-19T17:46:43.200782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_file = annotations_file.drop(labels=range(3), axis=0)\nannotations_file.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.204947Z","iopub.execute_input":"2022-02-19T17:46:43.20595Z","iopub.status.idle":"2022-02-19T17:46:43.228413Z","shell.execute_reply.started":"2022-02-19T17:46:43.205905Z","shell.execute_reply":"2022-02-19T17:46:43.227485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(annotations_file)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.229897Z","iopub.execute_input":"2022-02-19T17:46:43.231962Z","iopub.status.idle":"2022-02-19T17:46:43.242466Z","shell.execute_reply.started":"2022-02-19T17:46:43.231879Z","shell.execute_reply":"2022-02-19T17:46:43.241202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations_file.iloc[0,0]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.244422Z","iopub.execute_input":"2022-02-19T17:46:43.245706Z","iopub.status.idle":"2022-02-19T17:46:43.254816Z","shell.execute_reply.started":"2022-02-19T17:46:43.245611Z","shell.execute_reply":"2022-02-19T17:46:43.253393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dir_path = '../input/the-oxfordiiit-pet-dataset/images/images/'\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.256882Z","iopub.execute_input":"2022-02-19T17:46:43.258315Z","iopub.status.idle":"2022-02-19T17:46:43.263416Z","shell.execute_reply.started":"2022-02-19T17:46:43.258265Z","shell.execute_reply":"2022-02-19T17:46:43.262188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self,annotations_file,img_dir,transform = None):\n        self.annotations = annotations_file\n        self.img_dir = img_dir\n        self.transforms = transform\n        \n    #len simply returns the length of the dataset    \n    def __len__(self):\n        return len(self.annotations)\n    \n    #getitem returns the (x,y) pair at a particular index\n    def __getitem__(self,index):\n        img_path = f\"{self.img_dir}/{self.annotations.iloc[index,0]}.jpg\"\n        img = PIL.Image.open(img_path).convert('RGB')\n      \n        label = torch.tensor(int(self.annotations.iloc[index,1]))\n        \n        if self.transforms:\n            high_res_image = self.transforms[0](img)\n            low_res_image = self.transforms[1](img)\n            \n        return (high_res_image,low_res_image),label","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.265262Z","iopub.execute_input":"2022-02-19T17:46:43.266455Z","iopub.status.idle":"2022-02-19T17:46:43.278362Z","shell.execute_reply.started":"2022-02-19T17:46:43.266408Z","shell.execute_reply":"2022-02-19T17:46:43.277034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class res_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,stride):\n        super().__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,\n                                             padding = kernel_size // 2),\n                                   nn.BatchNorm2d(out_channels),\n                                   nn.PReLU(out_channels),\n                                   nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,\n                                             padding = kernel_size // 2),\n                                   nn.BatchNorm2d(out_channels))\n    \n    def forward(self,x):\n        return self.block(x)\n                                   \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.281786Z","iopub.execute_input":"2022-02-19T17:46:43.28466Z","iopub.status.idle":"2022-02-19T17:46:43.2933Z","shell.execute_reply.started":"2022-02-19T17:46:43.284612Z","shell.execute_reply":"2022-02-19T17:46:43.291738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class p_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size,stride,scale):\n        super().__init__()\n        self.block = nn.Sequential(nn.Conv2d(in_channels,out_channels,kernel_size = kernel_size,stride = stride,padding = kernel_size // 2),\n                                   nn.PixelShuffle(scale),\n                                   nn.PReLU(out_channels // (scale**2)))\n                                   \n    \n    def forward(self,x):\n        return self.block(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.297746Z","iopub.execute_input":"2022-02-19T17:46:43.299221Z","iopub.status.idle":"2022-02-19T17:46:43.308829Z","shell.execute_reply.started":"2022-02-19T17:46:43.299176Z","shell.execute_reply":"2022-02-19T17:46:43.30759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SRResNet(pl.LightningModule):\n    def __init__(self,lr,img_channels,img_size,depth,shuffle_scale,loss_type = 'MSE'):\n        super().__init__()\n        self.save_hyperparameters()      \n        \n        self.conv1 = nn.Sequential(nn.Conv2d(img_channels,64,kernel_size = 9, stride = 1,padding = 4),\n                                   nn.PReLU(64))\n        \n        self.residual_blocks = nn.ModuleList([\n                               res_block(64,64,3,1)\n                            \n                               for _ in range(depth)\n        ])\n        \n        self.conv2 = nn.Sequential(nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 3//2),\n                                   nn.BatchNorm2d(64))\n        \n        self.pshuffle = nn.ModuleList(\n                                       [\n                                           p_block(64,256,3,1,2),\n                                           p_block(64,256,3,1,2)                                           \n                                       ]\n        )\n        \n        self.conv3 = nn.Conv2d(64,3,kernel_size = 9,stride = 1, padding = 9//2)\n        \n    def forward(self,x):    \n        N,C,H,W = x.shape\n        H_out = H*2*self.hparams.shuffle_scale\n        W_out = W*2*self.hparams.shuffle_scale\n        \n        x = self.conv1(x)\n        conv1_output = x\n        for block in self.residual_blocks:\n            x  = block(x) + x\n        \n        x = self.conv2(x) + conv1_output\n        \n        for block in self.pshuffle:\n            x  = block(x)\n            \n        x = self.conv3(x)\n        x = x.view(N,C,H_out,W_out)     \n        \n        return x       \n    \n    def PSNR(self,img1, img2):\n        mse = torch.mean((img1 - img2) ** 2)\n        return -20 * torch.log10(255.0 / torch.sqrt(mse))\n    \n        \n    def loss_fn(self,img1,img2):\n        if (self.hparams.loss_type == \"PSNR\"):\n            return self.PSNR(img1,img2)\n        \n        elif (self.hparams.loss_type == \"MSE\"):\n            criterion = nn.MSELoss()\n            return criterion(img1,img2)\n        \n        elif (self.hparams.loss_type == \"MAE\"):\n            criterion = nn.L1Loss()\n            return criterion(img1,img2)\n    \n    def training_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        x_hat = self(low_res_image)\n        \n        loss = self.loss_fn(x_hat,high_res_image)\n        return loss\n    \n    \n    \n    def validation_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        x_hat = self(low_res_image)\n        \n        loss = self.loss_fn(x_hat,high_res_image)\n        self.log(\"val_loss\",loss)\n        \n    def test_step(self,batch,batch_idx):\n        (high_res_image,low_res_image),_ = batch\n        x_hat = self(low_res_image)\n        \n        loss = self.loss_fn(x_hat,high_res_image)\n        self.log(\"test_loss\",loss)\n\n        \n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(),lr = self.hparams.lr)\n        return optimizer\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.310629Z","iopub.execute_input":"2022-02-19T17:46:43.311445Z","iopub.status.idle":"2022-02-19T17:46:43.335999Z","shell.execute_reply.started":"2022-02-19T17:46:43.311385Z","shell.execute_reply":"2022-02-19T17:46:43.335007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    resize_transform = transforms.Compose([\n                                    transforms.Resize((320,320)),\n                                    transforms.ToTensor()\n                               ]\n    )\n\n    transform = transforms.Compose([\n                                    transforms.Resize((80,80)),\n                                    transforms.ToTensor()\n                               ]\n    )\n\n\n    dataset = CustomDataset(annotations_file = annotations_file,\n                            img_dir = dir_path ,\n                            transform = [resize_transform,transform])       \n\n    dataset_len = len(dataset)\n    train_set_len = int(0.7*dataset_len)\n    val_set_len = int(0.15*dataset_len)\n    test_set_len  = dataset_len - train_set_len - val_set_len\n\n    train_set,val_set,test_set = torch.utils.data.random_split(dataset,\n                                                               [train_set_len,val_set_len,test_set_len],\n                                                               generator=torch.Generator().manual_seed(43))\n\n    train_loader = DataLoader(train_set,batch_size = 16)\n    val_loader = DataLoader(val_set,batch_size = 16)\n    test_loader = DataLoader(test_set,batch_size = 16)        \n\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.33963Z","iopub.execute_input":"2022-02-19T17:46:43.340448Z","iopub.status.idle":"2022-02-19T17:46:43.367026Z","shell.execute_reply.started":"2022-02-19T17:46:43.3404Z","shell.execute_reply":"2022-02-19T17:46:43.366133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in test_loader:\n        (x1,x2), _ = batch\n        x1= x1[0:3]\n        x1.unsqueeze(0)\n        grid = torchvision.utils.make_grid(x1)\n        high_res_imgs = grid\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.368668Z","iopub.execute_input":"2022-02-19T17:46:43.369013Z","iopub.status.idle":"2022-02-19T17:46:43.721736Z","shell.execute_reply.started":"2022-02-19T17:46:43.368968Z","shell.execute_reply":"2022-02-19T17:46:43.720752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = pl.callbacks.EarlyStopping(monitor = 'val_loss',\n                                                       patience = 3,\n                                                       mode = 'min')\n\ncheckpoint = pl.callbacks.ModelCheckpoint(dirpath = 'saved_ckpts',\n                                          monitor = 'val_loss')\n\n\n\nmodel = SRResNet(lr = 1e-4,\n              img_channels = 3,\n              img_size = 80,\n              depth = 12,\n              shuffle_scale = 2,\n              loss_type = \"MAE\")\ntrainer = pl.Trainer(gpus = 1,\n                     precision = 16,\n                     callbacks = [early_stopping,checkpoint],\n                     max_epochs = 10)\n\ntrainer.fit(model,train_loader,val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T17:46:43.723313Z","iopub.execute_input":"2022-02-19T17:46:43.723633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SRResNet.load_from_checkpoint(checkpoint.best_model_path)\n\n\nwith torch.no_grad():\n    for batch in test_loader:\n        (x1,x2), _ = batch\n        x2 = x2[0:3]\n        output_imgs = model(x2)\n        output_imgs.unsqueeze(0)\n        grid = torchvision.utils.make_grid(output_imgs)\n        output_imgs = grid\n        break\n\n\ntrainer.test(model = model,dataloaders = test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.imshow(high_res_imgs.permute(1,2,0))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nplt.imshow(output_imgs.permute(1,2,0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}